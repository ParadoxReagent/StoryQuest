services:
  # Backend API Service
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: storyquest-backend
    ports:
      - "8000:8000"
    environment:
      # Database
      - DATABASE_URL=sqlite:///./data/storyquest.db

      # LLM Provider - Choose one: ollama, openai, anthropic, gemini, openrouter, lmstudio
      - LLM_PROVIDER=openrouter

      # Ollama configuration (if using local LLM)
      # - OLLAMA_BASE_URL=http://host.docker.internal:11434
      # - OLLAMA_MODEL=llama3.2:3b

      # OpenAI configuration (uncomment if using)
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
      # - OPENAI_MODEL=gpt-5.1-mini

      # Anthropic configuration (uncomment if using)
      # - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      # - ANTHROPIC_MODEL=claude-4-5-haiku-20241022

      # Gemini configuration (uncomment if using)
      # - GEMINI_API_KEY=${GEMINI_API_KEY}
      # - GEMINI_MODEL=gemini-2.5-flash

      # OpenRouter configuration (uncomment if using)
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - OPENROUTER_MODEL=google/gemini-2.5-flash
      # - OPENROUTER_SITE_URL=https://storyquest.local
      # - OPENROUTER_APP_NAME=StoryQuest

      # LM Studio configuration (uncomment if using)
      # - LMSTUDIO_BASE_URL=http://host.docker.internal:1234
      # - LMSTUDIO_MODEL=local-model

      # Safety configuration
      - USE_ENHANCED_FILTER=true
      - USE_MODERATION_API=false
      - LOG_VIOLATIONS=true
      - ENABLE_RATE_LIMITING=true

      # Python configuration
      - PYTHONUNBUFFERED=1
    volumes:
      # Persist database
      - backend-data:/app/data
      # Mount config (optional - for easy editing)
      - ./backend/config.yaml:/app/config.yaml:ro
    networks:
      - storyquest-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Frontend Web UI Service
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        # API URL - points to backend service
        - VITE_API_URL=http://localhost:8000
        # TTS URL - points to TTS service (Kokoro by default, or Chatterbox)
        - VITE_TTS_URL=http://localhost:8001
    container_name: storyquest-frontend
    ports:
      - "3000:8080"
    depends_on:
      - backend
    networks:
      - storyquest-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # =============================================================================
  # TTS SERVICES - Choose ONE of the following TTS services
  # =============================================================================
  #
  # Option 1: Kokoro TTS (DEFAULT) - Fast, lightweight, works great on CPU/Apple Silicon
  # Option 2: Chatterbox TTS - Higher quality but requires NVIDIA GPU for good performance
  #
  # To switch TTS providers:
  #   - Comment out the one you don't want
  #   - Uncomment the one you do want
  #   - Both use port 8001, so only ONE should be active at a time
  # =============================================================================

  # Kokoro TTS Service (DEFAULT) - Fast CPU/MPS performance
  # Recommended for: Mac (Apple Silicon), CPU-only systems, faster response times
  tts:
    build:
      context: ./tts-kokoro
      dockerfile: Dockerfile
    container_name: storyquest-tts
    ports:
      - "8001:8001"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTORCH_ENABLE_MPS_FALLBACK=1
    volumes:
      - tts-cache:/app/cache
    networks:
      - storyquest-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s

  # Chatterbox TTS Service (ALTERNATIVE) - Higher quality, requires NVIDIA GPU
  # Recommended for: Systems with NVIDIA CUDA GPU
  # To use: Comment out Kokoro above, uncomment this section
  # tts:
  #   build:
  #     context: ./tts-chatterbox
  #     dockerfile: Dockerfile
  #   container_name: storyquest-tts
  #   ports:
  #     - "8001:8001"
  #   environment:
  #     - PYTHONUNBUFFERED=1
  #   volumes:
  #     - tts-cache:/app/cache
  #   networks:
  #     - storyquest-network
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
  #     interval: 30s
  #     timeout: 30s
  #     retries: 3
  #     start_period: 120s

volumes:
  backend-data:
    name: storyquest-data
  tts-cache:
    name: storyquest-tts-cache

networks:
  storyquest-network:
    name: storyquest-network
    driver: bridge

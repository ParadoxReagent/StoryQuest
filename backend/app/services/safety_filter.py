"""
Safety filter for content moderation.
Phase 3: Core Story Engine Backend
"""

import logging
import re
from typing import List, Optional, Tuple

logger = logging.getLogger(__name__)


class SafetyFilter:
    """Content safety filter for kid-friendly story generation."""

    def __init__(self):
        """Initialize safety filter with banned words and patterns."""
        self.banned_words = self._load_banned_words()
        self.inappropriate_patterns = self._load_inappropriate_patterns()
        self.max_input_length = 200

    def _load_banned_words(self) -> List[str]:
        """
        Load list of banned words.

        Returns:
            List of banned words (lowercase)
        """
        # Basic list of inappropriate words for kids
        # In production, this would be loaded from a file or database
        return [
            "kill",
            "murder",
            "death",
            "die",
            "blood",
            "gore",
            "weapon",
            "gun",
            "knife",
            "sword",
            "fight",
            "attack",
            "hurt",
            "pain",
            "scary",
            "horror",
            "monster",
            "ghost",
            "zombie",
            "demon",
            "hell",
            "damn",
            "stupid",
            "idiot",
            "hate",
        ]

    def _load_inappropriate_patterns(self) -> List[re.Pattern]:
        """
        Load regex patterns for inappropriate content.

        Returns:
            List of compiled regex patterns
        """
        patterns = [
            # URLs
            r"https?://\S+",
            r"www\.\S+",
            # Email addresses
            r"\S+@\S+\.\S+",
            # Phone numbers
            r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b",
            # Addresses (basic pattern)
            r"\b\d+\s+\w+\s+(street|st|avenue|ave|road|rd|boulevard|blvd)\b",
        ]
        return [re.compile(pattern, re.IGNORECASE) for pattern in patterns]

    async def filter_user_input(self, text: str) -> Tuple[bool, str]:
        """
        Filter and validate user input.

        Args:
            text: User input text to filter

        Returns:
            Tuple of (is_safe: bool, sanitized_text_or_reason: str)
                - If safe: (True, sanitized_text)
                - If unsafe: (False, rejection_reason)
        """
        if not text or not text.strip():
            return False, "Input cannot be empty"

        # Check length
        if len(text) > self.max_input_length:
            return False, f"Input too long (max {self.max_input_length} characters)"

        # Sanitize text
        sanitized = text.strip()

        # Check for inappropriate patterns
        for pattern in self.inappropriate_patterns:
            if pattern.search(sanitized):
                logger.warning(f"Input rejected - matched pattern: {pattern.pattern}")
                return False, "Input contains inappropriate content (URLs, emails, or personal information)"

        # Check for banned words
        words = sanitized.lower().split()
        for word in words:
            # Remove punctuation for checking
            clean_word = re.sub(r'[^\w\s]', '', word)
            if clean_word in self.banned_words:
                logger.warning(f"Input rejected - banned word: {clean_word}")
                return False, f"Input contains inappropriate word: '{clean_word}'"

        return True, sanitized

    async def validate_llm_output(self, scene_text: str, choices: Optional[List[str]]) -> bool:
        """
        Validate LLM output for appropriateness.

        Args:
            scene_text: The scene text generated by LLM
            choices: List of choice options generated by LLM (optional for final turn)

        Returns:
            True if content is appropriate, False otherwise
        """
        # Check scene text
        scene_lower = scene_text.lower()
        for word in self.banned_words:
            if word in scene_lower:
                logger.warning(f"LLM output rejected - banned word in scene: {word}")
                return False

        # Check choices (if provided)
        if choices:
            for choice in choices:
                choice_lower = choice.lower()
                for word in self.banned_words:
                    if word in choice_lower:
                        logger.warning(f"LLM output rejected - banned word in choice: {word}")
                        return False

        # Check for negative sentiment (basic check)
        negative_indicators = ["sad", "cry", "afraid", "scared", "worried", "lonely", "lost"]
        negative_count = sum(1 for indicator in negative_indicators if indicator in scene_lower)

        # Allow some mild negative emotions (it's part of storytelling),
        # but reject if too many negative indicators
        if negative_count > 3:
            logger.warning(f"LLM output rejected - too negative ({negative_count} negative indicators)")
            return False

        return True

    def get_fallback_response(self, theme: str) -> Tuple[str, List[str]]:
        """
        Get a safe fallback response if LLM output is rejected.

        Args:
            theme: Story theme

        Returns:
            Tuple of (scene_text, choices)
        """
        fallback_scenes = {
            "space_adventure": (
                "You float peacefully in your spaceship, looking at the beautiful stars through the window. "
                "The computer beeps cheerfully, ready for your next command.",
                [
                    "Check the navigation system",
                    "Look at the star map",
                    "Take a moment to draw what you see"
                ]
            ),
            "magical_forest": (
                "You find yourself in a peaceful clearing filled with colorful flowers and friendly butterflies. "
                "A gentle breeze rustles the leaves above you.",
                [
                    "Pick some flowers",
                    "Follow the butterflies",
                    "Rest under a tree"
                ]
            ),
            "underwater_quest": (
                "You swim through clear, warm water, surrounded by colorful fish and coral. "
                "The sun's rays create beautiful patterns on the ocean floor.",
                [
                    "Follow the colorful fish",
                    "Explore the coral reef",
                    "Float and watch the bubbles"
                ]
            ),
            "dinosaur_discovery": (
                "You're in a lush valley where friendly dinosaurs munch on leaves and play together. "
                "A small dinosaur waves its tail at you in greeting.",
                [
                    "Wave back to the dinosaur",
                    "Look for interesting plants",
                    "Find a good spot to observe"
                ]
            ),
        }

        # Default fallback if theme not found
        default = (
            "You find yourself in a wonderful place filled with possibilities. "
            "Everything around you is peaceful and inviting.",
            [
                "Look around carefully",
                "Take a deep breath and think",
                "Choose a direction to explore"
            ]
        )

        scene, choices = fallback_scenes.get(theme, default)
        logger.info(f"Using fallback response for theme: {theme}")
        return scene, choices
